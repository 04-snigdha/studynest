Gradient Descent is an iterative optimization algorithm used to minimize a function by moving in the direction of the negative gradient.
Learning rate controls the step size. Too large may overshoot; too small may converge slowly.
Stochastic Gradient Descent uses small batches to approximate the gradient each step.
Backpropagation computes gradients for neural nets using the chain rule.
